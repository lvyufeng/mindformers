model:
  model_config:
    type: Gpt2Config
    seq_length: 2048
    vocab_size: 51200
    embedding_size: 12288
    num_layers: 96
    num_heads: 96
    expand_ratio: 4
    hidden_act: "fast_gelu"
    dropout_prob: 0.1
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    initializer_range: 0.02
    param_init_type: "float16"
    layernorm_dtype: "float32"
    softmax_dtype: "float16"
    compute_dtype: "float16"
    checkpoint_name_or_path: "gpt2_13b"
    eos_token: 50256
    use_relative_positions: False
    per_stage_layers: 3
  arch:
    type: GPT2LMHeadModel
