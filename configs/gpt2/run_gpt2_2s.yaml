base_config: [
  './task_config/context.yaml',
  './task_config/runner.yaml',
  './task_config/gpt2_dataset.yaml',
  './model_config/gpt2.yaml',
  '../__base__.yaml' ]

runner_config:
  epochs: 3
  batch_size: 2
  sink_size: 1
  sink_mode: True
  per_epoch_size: 1
  initial_epoch: 0
  has_trained_epoches: 0
  has_trained_steps: 0

parallel:
  parallel_mode: 1 # 0-dataset, 1-semi, 2-auto, 3-hybrid
  gradients_mean: False  # 是否在梯度的 AllReduce后执行平均算子，不支持单卡
  loss_repeated_mean: True
  full_batch: True # 是否在非数据并行模式下加载整个batch数据集
  search_mode: "sharding_propagation"  # 设置切分策略传播算法
  enable_parallel_optimizer: True  # 是否开启优化器并行
  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"  # 恢复训练的模型的切片

recompute_config:
  recompute: True  # 指定激活层是否切片保存
  parallel_optimizer_comm_recompute: False  # 指定由优化器切分产生的AllGather算子是否进行重计算
  mp_comm_recompute: True  # 指定由模型并行成分产生的通信算子是否进行重计算
  recompute_slice_activation: False  # 指定激活层是否切片保存

# 4 nodes 32 device num
parallel_config:  # 并行设置
  data_parallel: 4  # 数据并行数
  model_parallel: 2  # 模型/Tensor并行数
  pipeline_stage: 2  # 流水线并行数
  optimizer_shard: True  # 是否开启优化器并行
  micro_batch_num: 4  # 流水线并行时将MiniBatch切分成更细的MicroBatch的数目，流水线并行时生效，该值需≥流水线并行数
  vocab_emb_dp: True  # 是否配置embedding为数据并行
  gradient_aggregation_group: 4  # 优化器并行对应的梯度聚合数目

profile: False
use_parallel: True

seed: 0
resume_or_finetune_checkpoint: ""
# 使用分布式加载被切分的权重时请输入存放checkpoint的根目录，目录结构需满足 */{checkpoint_root_dir}/rank_*/checkpoint/**.ckpt
# 此时只需要写: resume_or_finetune_checkpoint: "*/{checkpoint_root_dir}", 只填根目录即可.


run_mode: 'train'
trainer:
  type: MaskedLanguageModelingTrainer
  model_name: 'gpt2'