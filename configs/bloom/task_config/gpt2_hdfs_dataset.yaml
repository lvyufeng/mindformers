train_dataset: &train_dataset
  data_loader:
    type: WikitextHDFSDataLoader
    dataset_dir: ""  # 默认HDFS的路径
    hosts: ""  # HDFS hosts 用于注册hdfs访问的网址
    user_name: "" # HDFS 用户名
    pull_data_file_number: 1 # 在线从HDFS上下载以.tokens结尾的文件数量
    endswith_words: ".tokens" # 拉取文件匹配的标识符号
    shuffle: True
    online_open: True # 打开则直接从HDFS上读取数据，不在copy到本地
    column_names: ["input_ids"]
  tokenizer:
    type: GPT2Tokenizer
    max_length: 1025
  input_columns: ["input_ids"]  # "input_mask", "label_ids"
  num_parallel_workers: 8
  python_multiprocessing: False
  drop_remainder: True
  batch_size: 8
  repeat: 1
  numa_enable: False
  prefetch_size: 1

train_dataset_task:
  type: CausalLanguageModelDataset
  dataset_config: *train_dataset
